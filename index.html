<html>
<head>
  <style>
    table, th, td {
      border: 1px solid black;
      border-collapse: collapse;
    }
  </style>
</head>
<body>
<h1>Group 6 Final Report</h1>
<h2>1) Introduction</h2>
<p>
  &emsp; a) Literature Review <br>
  &emsp; &emsp;   i)&emsp;Lu was concerned with flight prices as a time-series and <br>
  &emsp;&emsp;&emsp;  &emsp;when to buy/wait. Lu found the AdaBoost-DecisionTree <br>
  &emsp;&emsp;&emsp;  &emsp;algorithm had the best performance (61.35% than random <br>
  &emsp;&emsp;&emsp;  &emsp;purchase strategy) over the eight routes examined; K-means <br>
  &emsp;&emsp;&emsp;  &emsp;and EM to address outliers. <br>
  &emsp; &emsp;ii) &emsp;S. Gupta and N. Gupta used <q>a combination of decision tree <br>
  &emsp;&emsp;&emsp;  &emsp;algorithms, k Nearest Neighbors (KNN), and linear <br>
  &emsp;&emsp;&emsp;  &emsp;regression</q> on historical trends to forecast flight prices. <br>
  &emsp;&emsp;&emsp;  &emsp;Their Random Forest Regressor and Decision Tree Regressor <br>
  &emsp;&emsp;&emsp;  &emsp;had r2 scores of .81 and .67 respectively. <br>
  &emsp;&emsp;iii) &emsp;Tziridis et al. evaluated different regression models <br>
  &emsp;&emsp;&emsp;  &emsp;including random forests and support vector machines on a <br>
  &emsp;&emsp;&emsp;  &emsp;dataset of over 1,000 flights, focusing on how departure time <br>
  &emsp;&emsp;&emsp;  &emsp;and the number of stops a given flight had would impact the <br>
  &emsp;&emsp;&emsp;  &emsp;prediction accuracy. They found that the bagging regression <br>
  &emsp;&emsp;&emsp;  &emsp;tree model had the highest accuracy of 87.42%. <br>
  &emsp; b) Dataset Description <br>
  &emsp; &emsp;   i)&emsp;The data contains ticket prices of 95 flights with route SFO-LAX <br>
  &emsp;&emsp;&emsp;  &emsp;from Oct 11 2023 to Oct 20 2023. There are 57 prices provided for <br>
  &emsp;&emsp;&emsp;  &emsp;each of the 95 flights, corresponding to the number of days before <br>
  &emsp;&emsp;&emsp;  &emsp;departure that the flight had sold for.<br>
  &emsp; c) Dataset Link: <a href="https://www.kaggle.com/code/andrewrliu/airfare-predictor-pt-1/input">Airfare Predictor (pt.1)</a>
</p>
<h2>2) Problem Description and Motivation</h2>
<p>
  The distribution of flight prices is inherently non-linear and does not follow an easily predictable <br>
  distribution. The goal of this paper is to create a machine learning approach to the problem of the <br>
  distributions of ticket flight prices when given partial information about the distribution. In other words, <br>
  by giving an earlier part of the distribution, the latter part of the distribution will be predicted allowing <br>
  for educated choices on when to purchase a flight ticket at the cheapest time. We will provide the models with <br>
  ticket prices for 14-57 (44 total days), 25-57 (33 total days), and 36-57 (22 total days) days before departure <br>
  for 76 of the 95 flights (80% of the flights). 

</p>
<h2>3) Methods</h2>
<p>
  &emsp; a) 1+ Preprocessing methods <br>
  &emsp; &emsp;   i)&emsp; Visualization<br>
  &emsp;&emsp;&emsp;  &emsp; For each flight, a data point was created containing a column of ticket prices<br>
  &emsp;&emsp;&emsp;  &emsp; and a column containing the number of days. This matrix was used to create a<br>
  &emsp;&emsp;&emsp;  &emsp; scatterplot of prices over different number of days befor the plane's departure<br>
  &emsp; &emsp;ii) &emsp; Removing Outliers<br>
  &emsp;&emsp;&emsp;  &emsp; After visualizing the data, some of the scatter plots showed signs of outliers. <br>
  &emsp;&emsp;&emsp;  &emsp; Most of these outliers were located in the couple of days leading up to each flight. <br>
  &emsp;&emsp;&emsp;  &emsp; Consequently, we decided to remove the data one and two days in advance for each flight. <br>
  &emsp;&emsp;&emsp;  &emsp; This left us with 55 data points (3-57 days before departure) for each of the 95 flights.<br>
  <!--<h2>Flight Price with Respect to Days Before Departure</h2><br>
  <img src="./docs/assets/css/Screenshot_3-7-2024_8394_colab.research.google.png" title="Data vs. Processed Data" width="75%" height="75%"><br><br>-->
  &emsp; b) 3+ Algorithms/Models <br>
  &emsp;  &emsp;  i) &emsp;Direct Architecture<br>
  &emsp;&emsp;&emsp;  &emsp; This architecture takes the direct approach to the problem, taking each of the days inside <br>
  &emsp;&emsp;&emsp;  &emsp; of the prior distribution as a feature to predict the remaining days inside of the distribution <br>
  &emsp;&emsp;&emsp;  &emsp; as an output. Thus if one wished to predict the distribution given prices for 25-57 days before <br>
  &emsp;&emsp;&emsp;  &emsp; departure, then each one of those 33 days will be a feature, with 22 outputs (3-24 days before departure). <br>
  &emsp;&emsp;&emsp;  &emsp; This approach however suffers from the fact that with only 95 total flights, there are a great deal of <br>
  &emsp;&emsp;&emsp;  &emsp; features compared to data points. In this implementation various different algorithms were attempted with <br>
  &emsp;&emsp;&emsp;  &emsp; those being Multi-level perceptron regressor, Random Forest Regressor, and Linear Regression with polynomial <br>
  &emsp;&emsp;&emsp;  &emsp; features and Lasso regularization. <br>
  &emsp;  &emsp; ii) &emsp;Iterative Architecture<br>
  &emsp;&emsp;&emsp;  &emsp; This architecture attempts to understand the relationships between the points in an iterative <br>
  &emsp;&emsp;&emsp;  &emsp; manner. For example, in order to predict the distribution given prices for 25-57 days before <br>
  &emsp;&emsp;&emsp;  &emsp; departure, it would be ‘easier’ to predict the price at 24 days before departure rather than <br>
  &emsp;&emsp;&emsp;  &emsp; predicting the entire distribution at once. Thus this approach trains multiple models with a step <br>
  &emsp;&emsp;&emsp;  &emsp; of 1 day until reaching the endpoint (3 days before departure), progressively increasing the number <br>
  &emsp;&emsp;&emsp;  &emsp; of days in the X set in the form of adding a new feature. Overall this architecture has the major drawback <br>
  &emsp;&emsp;&emsp;  &emsp; of requiring a different model to understand the relationship between each point and the points prior to it, <br>
  &emsp;&emsp;&emsp;  &emsp; which can make this approach much more expensive compared to the direct approach. In the implementation of <br>
  &emsp;&emsp;&emsp;  &emsp; this architecture Random Forest Regressor and Multi-level perceptron versions were used. <br>
   &emsp;  &emsp; iii) &emsp;Iterative Architecture with Soft-Clustering<br>
  &emsp;&emsp;&emsp;  &emsp; This architecture uses the iterative architecture, however in addition to each model there <br>
  &emsp;&emsp;&emsp;  &emsp; is an unsupervised soft-classification scheme (Gaussian Mixture Model in this case) <br>
  &emsp;&emsp;&emsp;  &emsp; which was trained to recognize different types of distributions. For each one of these <br>
  &emsp;&emsp;&emsp;  &emsp; gaussians, a separate random forest regressor model was fitted, resulting in 3 random <br>
  &emsp;&emsp;&emsp;  &emsp; forest models per step. By taking a weighted average for each point using the <br>
  &emsp;&emsp;&emsp;  &emsp; confidence percentages for each gaussian using the current X, this method promises <br>
  &emsp;&emsp;&emsp;  &emsp; greater understanding compared to simply using the iterative approach as it will use the <br>
  &emsp;&emsp;&emsp;  &emsp; soft-clustering understanding of distributions alongside the supervised learning <br>
  &emsp;&emsp;&emsp;  &emsp; predictive elements to create an ideal model at the  cost to performance given the fact <br>
  &emsp;&emsp;&emsp;  &emsp; that for each step a GMM will need to be fitted, alongside 3 Random Forest <br>
  &emsp;&emsp;&emsp;  &emsp; Regressions.<br>

</p>
<h2>4) Results and Discussion</h2>
<p>
  Accuracy measures the percentage of predicted prices that are within $30 from the actual price for all <br>
  19 tested flights. <br>
  &emsp; a) 44 point split <br>
  &emsp;  &emsp;  i) &emsp;Direct Approach Results<br>
  &emsp;  &emsp; &emsp;  1) &emsp;MLP<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 755.99<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 79.90%<br>
  <img src="./docs/assets/css/44/dmlp81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/44/dmlp87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  2) &emsp;RFR<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 989.08<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 80.86%<br>
   <img src="./docs/assets/css/44/drf81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/44/drf87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  3) &emsp;Lasso<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 5795.66<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 60.29%<br>
  <img src="./docs/assets/css/44/dl81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/44/dl87.png" title="MLP4481"><br>
  &emsp;  &emsp; ii) &emsp;Iterative Architecture and Iterative with Soft-clustering Results<br>
  &emsp;  &emsp; &emsp;  1) &emsp;MLP<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 1095.71<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 72.63%<br>
  <img src="./docs/assets/css/44/imlp81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/44/imlp87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  2) &emsp;RFR<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 1092.52<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 80.53%<br>
  <img src="./docs/assets/css/44/irf81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/44/irf87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  3) &emsp;GMM<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 1672.85<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 77.37%<br>
  <img src="./docs/assets/css/44/igmm81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/44/igmm87.png" title="MLP4481"><br>
 &emsp; b) 33 point split<br>
  &emsp;  &emsp;  i) &emsp;Direct Approach Results<br>
  &emsp;  &emsp; &emsp;  1) &emsp;MLP<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 632.74<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy 82.54%<br>
  <img src="./docs/assets/css/33/dmlp81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/33/dmlp87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  2) &emsp;RFR<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 1024.48<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 80.14%<br>
  <img src="./docs/assets/css/33/drf81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/33/drf87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  3) &emsp;Lasso<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 4314.65<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 65.79%<br>
  <img src="./docs/assets/css/33/dl81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/33/dl87.png" title="MLP4481"><br>
  &emsp;  &emsp; ii) &emsp;Iterative Architecture and Iterative with Soft-clustering Results<br>
  &emsp;  &emsp; &emsp;  1) &emsp;MLP<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 1959.22<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 61.96%<br>
  <img src="./docs/assets/css/44/imlp81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/44/imlp87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  2) &emsp;RFR<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 1001.03<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 78.23%<br>
  <img src="./docs/assets/css/33/irf81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/33/irf87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  3) &emsp;GMM<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 1530.63<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 76.08%<br>
  <img src="./docs/assets/css/33/igmm81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/33/igmm87.png" title="MLP4481"><br>
  &emsp; c) 22 point split <br>
  &emsp;  &emsp;  i) &emsp;Direct Approach Results<br>
  &emsp;  &emsp; &emsp;  1) &emsp;MLP<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 659.61<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 79.59%<br>
  <img src="./docs/assets/css/22/dmlp81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/22/dmlp87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  2) &emsp;RFR<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 991.64<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 77.03%<br>
  <img src="./docs/assets/css/22/drf81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/22/drf87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  3) &emsp;Lasso<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 2755.09<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 70.81%<br>
  <img src="./docs/assets/css/22/dl81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/22/dl87.png" title="MLP4481"><br>
  &emsp;  &emsp; ii) &emsp;Iterative Architecture and Iterative with Soft-clustering Results<br>
  &emsp;  &emsp; &emsp;  1) &emsp;MLP<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 1808.55<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 62.36%<br>
  
  <img src="./docs/assets/css/22/imlp81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/22/imlp87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  2) &emsp;RFR<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 1046.69<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 75.44%<br>
  
  <img src="./docs/assets/css/22/irf81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/22/irf87.png" title="MLP4481"><br>
  &emsp;  &emsp; &emsp;  3) &emsp;GMM<br>
  &emsp;&emsp;&emsp;  &emsp; The MSE is 2043.51<br>
  &emsp;&emsp;&emsp;  &emsp; The accuracy is 73.05%<br>
  
  <img src="./docs/assets/css/22/igmm81.png" title="MLP4481"><br>
  <img src="./docs/assets/css/22/igmm87.png" title="MLP4481"><br>
  &emsp; b) Next Steps<br>
  &emsp;&emsp; &emsp;The next steps are primarily with optimizing the individual architectures involved. For <br>
  &emsp;&emsp; &emsp;most of them pre-defined hyper-parameters such as max_iters, and learning rate were  <br>
  &emsp;&emsp; &emsp;not experimented with to find which results in the best accuracy. In addition, more flights   <br>
  &emsp;&emsp; &emsp;might provide a better model, thus a future step might involve looking for a more    <br>
  &emsp;&emsp; &emsp;expansive dataset. In addition, especially for scenarios where there are not many days     <br>
  &emsp;&emsp; &emsp;left to predict inside of the distribution, feature reduction might be undertaken to prevent      <br>
  &emsp;&emsp; &emsp;the model from overfitting and only use the most important features inside of the dataset.     <br>

  &emsp; c) Analysis <br>
  &emsp;&emsp; &emsp;The primary drawback encountered was a decrease in accuracy and an increase in error for <br>
  &emsp;&emsp; &emsp;all the iterative models as the number of days in advance given to them decreased which makes <br>
  &emsp;&emsp; &emsp;sense given that an error in one of the early points would lead to a cascading error further <br>
  &emsp;&emsp; &emsp;down the line. Overall, while theoretically interesting, the iterative approach with soft-clustering <br>
  &emsp;&emsp; &emsp;proved to be underwhelming, scoring worse than the iterative approach with random forest thus meaning <br>
  &emsp;&emsp; &emsp;that with similar datasets this approach should not be tried with our data. <br> <br>
  &emsp;&emsp; &emsp;For iterative results, random forest proved to be universally the best in both MSE and <br>
  &emsp;&emsp; &emsp;accuracy compared to the MLP regressor, especially when given less days in advance. For direct <br>
  &emsp;&emsp; &emsp;results, lasso proved to be consistently the worst, however got more accurate as it was given <br>
  &emsp;&emsp; &emsp;less days in advance, likely speaking to overfitting that may have occurred with the deluge of <br>
  &emsp;&emsp; &emsp;features that the model is given when there are very few days left to predict. Overall for the <br>
  &emsp;&emsp; &emsp;direct approach the MLP regressor was able to have the best MSE, and comparable or better results <br>
  &emsp;&emsp; &emsp;compared to random forest. The direct approach seemed more resilient than the iterative approach when <br>
  &emsp;&emsp; &emsp;it comes to changing the number of days provided, however unlike the iterative approaches, one must <br>
  &emsp;&emsp; &emsp;re-train a direct approach model, rather than use the existing model reducing the flexibility of <br>
  &emsp;&emsp; &emsp;application.


</p>
<h2>5) References</h2>
<p>
[1] J. Lu, <q>Machine learning modeling for time series problem: Predicting <br>
  &emsp; flight ticket prices,</q> arXiv.org, https://arxiv.org/abs/1705.07205 <br>
  &emsp; (accessed Jun. 11, 2024). <br>
[2] S. Gupta and N. Gupta, Flight fare prediction using machine learning, <br>
  &emsp; https://www.researchgate.net/publication/380296130_Flight_Fare_Pr<br>
  &emsp; ediction_Using_Machine_Learning (accessed Jun. 11, 2024).<br>
[3] K. Tziridis, Th. Kalampokas, G. A. Papakostas, and K. I. Diamantaras, <br>
  &emsp; Airfare prices prediction using Machine Learning Techniques, <br>
  &emsp; https://ieeexplore.ieee.org/document/8081365/ (accessed Jun. 11, 2024).
</p>
<h2>Gantt Chart</h2>
<a href="https://docs.google.com/spreadsheets/d/1p2POTEbhSWqfBzq0sjRmW3fbHE6IvLSu8YKhsBswh4A/edit?gid=1989857755#gid=1989857755">Gantt Chart</a>
<h2>Contribution Table</h2>
<table>
  <tr>
    <th>Name</th>
    <th>Proposal Contributions</th>
    <th>Midterm Contributions</th>
    <th>Final Contributions</th>
  </tr>
  <tr>
    <td> Francesco Cascone </td>
    <td> Gathering References, Writing the Report</td>
    <td> Preprocessing and Visualizing Data </td>
    <td> Direct Archtiecture, Visualizations </td>
  </tr>
  <tr>
    <td> William Hudson </td>
    <td> Video Presentation</td>
    <td> Github Pages, Prediction Analysis </td>
    <td> Final Report, Direct Archtiecture, Github Pages </td>
  </tr>
  <tr>
    <td> Viabhav Patel </td>
    <td> Gathering References, Writing the Report</td>
    <td> Preprocessing Data, Running Random Forests, Running DNN </td>
    <td> Video Presentation </td>
  </tr>
  <tr>
    <td> Matthew Spenney </td>
    <td> Github Repository, Github Pages</td>
    <td> Github Pages, Prediction Analysis </td>
    <td> Iterative Architectures, Github Pages </td>
  </tr>
  <tr>
    <td> Jonathan Zhang </td>
    <td> Writing the Report</td>
    <td> Running Random Forests </td>
    <td> Final Report, and Video Slides </td>
  </tr>
</table>
</body>
</html>
