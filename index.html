<html>
<head>
  <style>
    table, th, td {
      border: 1px solid black;
      border-collapse: collapse;
    }
  </style>
</head>
<body>
<h1>Group 6 Final Report</h1>
<h2>1) Introduction</h2>
<p>
  &emsp; a) Literature Review <br>
  &emsp; &emsp;   i)&emsp;Lu was concerned with flight prices as a time-series and <br>
  &emsp;&emsp;&emsp;  &emsp;when to buy/wait. Lu found the AdaBoost-DecisionTree <br>
  &emsp;&emsp;&emsp;  &emsp;algorithm had the best performance (61.35% than random <br>
  &emsp;&emsp;&emsp;  &emsp;purchase strategy) over the eight routes examined; K-means <br>
  &emsp;&emsp;&emsp;  &emsp;and EM to address outliers. <br>
  &emsp; &emsp;ii) &emsp;S. Gupta and N. Gupta used <q>a combination of decision tree <br>
  &emsp;&emsp;&emsp;  &emsp;algorithms, k Nearest Neighbors (KNN), and linear <br>
  &emsp;&emsp;&emsp;  &emsp;regression</q> on historical trends to forecast flight prices. <br>
  &emsp;&emsp;&emsp;  &emsp;Their Random Forest Regressor and Decision Tree Regressor <br>
  &emsp;&emsp;&emsp;  &emsp;had r2 scores of .81 and .67 respectively. <br>
  &emsp;&emsp;iii) &emsp;Tziridis et al. evaluated different regression models <br>
  &emsp;&emsp;&emsp;  &emsp;including random forests and support vector machines on a <br>
  &emsp;&emsp;&emsp;  &emsp;dataset of over 1,000 flights, focusing on how departure time <br>
  &emsp;&emsp;&emsp;  &emsp;and the number of stops a given flight had would impact the <br>
  &emsp;&emsp;&emsp;  &emsp;prediction accuracy. They found that the bagging regression <br>
  &emsp;&emsp;&emsp;  &emsp;tree model had the highest accuracy of 87.42%. <br>
  &emsp; b) Dataset Description <br>
  &emsp; &emsp;   i)&emsp;The data will contain prices of flights over time, consisting <br>
  &emsp;&emsp;&emsp;  &emsp;of fixed destinations and dates. The data contains information <br>
  &emsp;&emsp;&emsp;  &emsp;for United Airfare flight SFO-LAX <br>
  &emsp; c) Dataset Link: <a href="https://www.kaggle.com/code/andrewrliu/airfare-predictor-pt-1/input">Airfare Predictor (pt.1)</a>
</p>
<h2>2) Problem Description and Motivation</h2>
<p>
  The distribution of flight prices is inherently non-linear and does not follow an easily predictable distribution. The goal of this paper is to create a machine learning approach to the problem of the distributions of ticket flight prices when given partial information about the distribution. In other words, by giving an earlier part of the distribution, the latter part of the distribution will be predicted allowing for educated choices on when to purchase a flight ticket at the cheapest time.

</p>
<h2>3) Methods</h2>
<p>
  &emsp; a) Preprocessing methods <br>
  &emsp; &emsp;   i)&emsp; Visualization<br>
  &emsp;&emsp;&emsp;  &emsp; For each flight, a data point was created containing a column of ticket prices<br>
  &emsp;&emsp;&emsp;  &emsp; and a column containing the number of days. This matrix was used to create a<br>
  &emsp;&emsp;&emsp;  &emsp; scatterplot of prices over different number of days befor the plane's departure<br>
  &emsp; &emsp;ii) &emsp; Removing Outliers<br>
  &emsp;&emsp;&emsp;  &emsp; After visualizing the data, some of the scatterplots showed signs of outliers.<br>
  &emsp;&emsp;&emsp;  &emsp; We created a replica of the data with outliers removed. This creates gaps in the data,<br>
  &emsp;&emsp;&emsp;  &emsp; but keeps the average flight price from becoming skewed, especially by high prices.<br>
   &emsp;&emsp;&emsp;  &emsp; This is important for accurately finding less than average prices<br>
  <h2>Flight Price with Respect to Days Before Departure</h2><br>
  <img src="./docs/assets/css/Screenshot_3-7-2024_8394_colab.research.google.png" title="Data vs. Processed Data" width="75%" height="75%"><br><br>
  &emsp; b) Supervised Learning methods <br>
  &emsp;  &emsp;  i) &emsp;Direct Architecture<br>
  &emsp;&emsp;&emsp;  &emsp; This architecture takes the direct approach to the problem, taking each of the days  <br>
  &emsp;&emsp;&emsp;  &emsp; inside of the prior distribution as a feature to predict the remaining days inside of the  <br>
  &emsp;&emsp;&emsp;  &emsp; distribution as an output. Thus if one wished to predict the distribution using the <br>
  &emsp;&emsp;&emsp;  &emsp; information up to 33 days from the flight, then each one of those 22 days will be a  <br>
  &emsp;&emsp;&emsp;  &emsp; feature, with 31 outputs. This approach however suffers from the fact that with only 95  <br>
  &emsp;&emsp;&emsp;  &emsp; data points in total there exists a great deal of features compared to data points. In this   <br>
  &emsp;&emsp;&emsp;  &emsp; implementation various different algorithms were attempted with those being Multi-level    <br>
  &emsp;&emsp;&emsp;  &emsp; perceptron regressor, Random Forest Regressor, and Linear Regression with polynomial    <br>
  &emsp;&emsp;&emsp;  &emsp; features and Lasso regularization.    <br>
  &emsp;  &emsp; ii) &emsp;Iterative Architecture<br>
  &emsp;&emsp;&emsp;  &emsp; This architecture attempts to understand the relationships between the points in an<br>
  &emsp;&emsp;&emsp;  &emsp; iterative manner, by the assumption that if one has the prices from days 33 to 55, then <br>
  &emsp;&emsp;&emsp;  &emsp; predicting the price on day 32 would be ‘easier’ compared to predicting the entire <br>
  &emsp;&emsp;&emsp;  &emsp; distribution at one time. Thus this approach trains multiple models with a step of 1 day <br>
  &emsp;&emsp;&emsp;  &emsp; until reaching the endpoint (day 3), progressively increasing the number of days in the X <br>
  &emsp;&emsp;&emsp;  &emsp; set in the form of adding a new feature. Overall this architecture has the major drawback <br>
  &emsp;&emsp;&emsp;  &emsp; of requiring a different model to understand the relationship between each point and the <br>
  &emsp;&emsp;&emsp;  &emsp; points prior to it, which can make this approach much more expensive compared to the <br>
  &emsp;&emsp;&emsp;  &emsp; direct approach. In the implementation of this architecture Random Forest Regressor<br>
  &emsp;&emsp;&emsp;  &emsp; and Multi-level perceptron versions were used.<br>
   &emsp;  &emsp; iii) &emsp;Iterative Architecture with Soft-Clustering<br>
  &emsp;&emsp;&emsp;  &emsp; This architecture uses the iterative architecture, however in addition to each model there <br>
  &emsp;&emsp;&emsp;  &emsp; is an unsupervised soft-classification scheme (Gaussian Mixture Model in this case) <br>
  &emsp;&emsp;&emsp;  &emsp; which was trained to recognize different types of distributions. For each one of these <br>
  &emsp;&emsp;&emsp;  &emsp; gaussians, a separate random forest regressor model was fitted, resulting in 3 random <br>
  &emsp;&emsp;&emsp;  &emsp; forest models per step. By taking a weighted average for each point using the <br>
  &emsp;&emsp;&emsp;  &emsp; confidence percentages for each gaussian using the current X, this method promises <br>
  &emsp;&emsp;&emsp;  &emsp; greater understanding compared to simply using the iterative approach as it will use the <br>
  &emsp;&emsp;&emsp;  &emsp; soft-clustering understanding of distributions alongside the supervised learning <br>
  &emsp;&emsp;&emsp;  &emsp; predictive elements to create an ideal model at the  cost to performance given the fact <br>
  &emsp;&emsp;&emsp;  &emsp; that for each step a GMM will need to be fitted, alongside 3 Random Forest <br>
  &emsp;&emsp;&emsp;  &emsp; Regressions.<br>

</p>
<h2>4) Results and Discussion</h2>
<p>
  &emsp; a) Results from Supervized Learning algorithms <br>
  &emsp;  &emsp;  i) &emsp;Random Tree Regression<br>
  &emsp;&emsp;&emsp;  &emsp; As shown in the visualization, running Random Forest Regression on the testing<br>
  &emsp;&emsp;&emsp;  &emsp; flights caused the model to predict the flight price would stay around the same<br>
  &emsp;&emsp;&emsp;  &emsp; price. This is not a helpful prediction because it is not picking up the changes<br>
  &emsp;&emsp;&emsp;  &emsp; in price we see for individual flights. This is happening because ticket prices<br>
  &emsp;&emsp;&emsp;  &emsp; with respect to days till departure follows a different function for each flight.<br>
  &emsp;&emsp;&emsp;  &emsp; The prices for some flights increase, while the price for other flights decrease, so<br>
  &emsp;&emsp;&emsp;  &emsp; the algorithm tries to optimize this behavior by predicting the price at the average.<br>
  &emsp;&emsp;&emsp;  &emsp; Our R^2 value is: -0.02, our (non-normalized) MSE is 2919, or in other words the typical error<br>
  &emsp;&emsp;&emsp;  &emsp; is 54 dollars. <br>

  
  &emsp;  &emsp; ii) &emsp;Deep Neural Network<br>
  &emsp;&emsp;&emsp;  &emsp; The neural network predicted a similar result to the Random Tree Regression.<br>
  &emsp;&emsp;&emsp;  &emsp; Overall the model was able to quickly converge with a fairly stagnant training loss<br>
  &emsp;&emsp;&emsp;  &emsp; after very few epochs a certain complexity of model, and deep neural networks are able to present <br>
  &emsp;&emsp;&emsp;  &emsp;Overall the validation error shows that the issue being faced <br>
  &emsp;&emsp;&emsp;  &emsp;is not under/over fitting, rather that the issue is inherent <br>
  &emsp;&emsp;&emsp;  &emsp; to the methodology and the problem being faced.<br>
  &emsp;&emsp;&emsp;  &emsp; Our R^2 is 0.01, MSE (non-normalized) is 2837 so our values vary by about 53 dollars.<br>
  &emsp;&emsp;&emsp;  &emsp;The accuracy score within $30, as for the Random Tree Regression, was 0.367.<br><br>
  &emsp;  &emsp;  b) &emsp;Next Steps<br>
  &emsp;&emsp;&emsp;  &emsp;These results suggest that we should attempt a different approach. Rather <br>
  &emsp;&emsp;&emsp;  &emsp;than producing a model predicts the ticket price given only the number of days <br>
  &emsp;&emsp;&emsp;  &emsp;in advance of the ticket purchase, we will try to produce a model that can  <br>
  &emsp;&emsp;&emsp;  &emsp;predict the rest of the distribution of ticket prices given some of the data. <br>
  &emsp;&emsp;&emsp;  &emsp;That is, we want to create a model trained on a subset of the 95 flight distributions <br>
  &emsp;&emsp;&emsp;  &emsp;that can predict a new flight distribution based on some information about the new <br>
  &emsp;&emsp;&emsp;  &emsp;flight distribution. For example, given the ticket prices of a new flight for, say, <br>
  &emsp;&emsp;&emsp;  &emsp;35-57 days in advance, the model should be able to predict how the rest of the <br>
  &emsp;&emsp;&emsp;  &emsp;distribution will look like with certain probability.  <br>
 <br>
  &emsp;&emsp;&emsp;  &emsp;This would still be practical for a customer that tracks flight  <br>
  &emsp;&emsp;&emsp;  &emsp;prices 35-57 days in advance (which could be done with a web scraper)  <br>
  &emsp;&emsp;&emsp;  &emsp;as they could obtain a prediction for when the prices would be cheapest. <br>
  &emsp;&emsp;&emsp;  &emsp;The customer could even run the model every day, feeding it one more data <br>
  &emsp;&emsp;&emsp;  &emsp;point every successive day which would enable a more accurate prediction <br>

<h2>Visualiztion of Random Tree Predicted Prices Compared to original</h2>
<img src="./randomForest.png" title="Random Tree Prediction"><br>
  
<h2>Visualization for DNN Model</h2>
<img src="./DNN_2Results.png" title="DNN Visualization"><br>
<img src="./DNN_2Loss.png" title="DNN Loss during Optimization">
</p>
<h2>5) References</h2>
<p>
[1] J. Lu, <q>Machine learning modeling for time series problem: Predicting <br>
  &emsp; flight ticket prices,</q> arXiv.org, https://arxiv.org/abs/1705.07205 <br>
  &emsp; (accessed Jun. 11, 2024). <br>
[2] S. Gupta and N. Gupta, Flight fare prediction using machine learning, <br>
  &emsp; https://www.researchgate.net/publication/380296130_Flight_Fare_Pr<br>
  &emsp; ediction_Using_Machine_Learning (accessed Jun. 11, 2024).<br>
[3] K. Tziridis, Th. Kalampokas, G. A. Papakostas, and K. I. Diamantaras, <br>
  &emsp; Airfare prices prediction using Machine Learning Techniques, <br>
  &emsp; https://ieeexplore.ieee.org/document/8081365/ (accessed Jun. 11, 2024).
</p>
<h2>Gantt Chart</h2>
<a href="https://docs.google.com/spreadsheets/d/1p2POTEbhSWqfBzq0sjRmW3fbHE6IvLSu8YKhsBswh4A/edit?gid=1989857755#gid=1989857755">Gantt Chart</a>
<h2>Contribution Table</h2>
<table>
  <tr>
    <th>Name</th>
    <th>Proposal Contributions</th>
    <th>Midterm Contributions</th>
    <th>Midterm Contributions</th>
  </tr>
  <tr>
    <td> Francesco Cascone </td>
    <td> Gathering References, Writing the Report</td>
    <td> Preprocessing and Visualizing Data </td>
    <td> Direct Archtiecture, Visualizations </td>
  </tr>
  <tr>
    <td> William Hudson </td>
    <td> Video Presentation</td>
    <td> Github Pages, Prediction Analysis </td>
    <td> Final Report, Direct Archtiecture, Github Pages </td>
  </tr>
  <tr>
    <td> Viabhav Patel </td>
    <td> Gathering References, Writing the Report</td>
    <td> Preprocessing Data, Running Random Forests, Running DNN </td>
    <td> Video Presentation </td>
  </tr>
  <tr>
    <td> Matthew Spenney </td>
    <td> Github Repository, Github Pages</td>
    <td> Github Pages, Prediction Analysis </td>
    <td> Iterative Architectures, Github Pages </td>
  </tr>
  <tr>
    <td> Jonathan Zhang </td>
    <td> Writing the Report</td>
    <td> Running Random Forests </td>
    <td> Final Report, and Video Slides </td>
  </tr>
</table>
</body>
</html>
